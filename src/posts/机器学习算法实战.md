---
title: "机器学习算法实战"
excerpt: "深入学习机器学习核心算法，从理论到实践构建智能应用。"
author: "CodeBuddy"
category: "机器学习"
tags: ["机器学习", "Python", "算法", "数据科学"]
publishedAt: "2024-04-28"
updatedAt: "2024-04-28"
readTime: 30
coverImage: "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
isPublished: true
pinned: true
---

# 机器学习算法实战

机器学习是人工智能的核心技术，本文将深入探讨常用机器学习算法的原理和实现。

## 环境准备

### 核心库安装

```python
# requirements.txt
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
matplotlib==3.7.2
seaborn==0.12.2
jupyter==1.0.0
plotly==5.15.0
xgboost==1.7.6
lightgbm==4.0.0
tensorflow==2.13.0
torch==2.0.1
```

## 监督学习算法

### 线性回归实现

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

class LinearRegressionFromScratch:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.costs = []
    
    def fit(self, X, y):
        # 初始化参数
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # 梯度下降
        for i in range(self.n_iterations):
            # 前向传播
            y_predicted = np.dot(X, self.weights) + self.bias
            
            # 计算损失
            cost = (1 / (2 * n_samples)) * np.sum((y_predicted - y) ** 2)
            self.costs.append(cost)
            
            # 计算梯度
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)
            
            # 更新参数
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        return np.dot(X, self.weights) + self.bias
```

### 逻辑回归实现

```python
class LogisticRegressionFromScratch:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.costs = []
    
    def _sigmoid(self, z):
        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))
    
    def fit(self, X, y):
        # 初始化参数
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # 梯度下降
        for i in range(self.n_iterations):
            # 前向传播
            linear_model = np.dot(X, self.weights) + self.bias
            y_predicted = self._sigmoid(linear_model)
            
            # 计算损失
            cost = (-1 / n_samples) * np.sum(y * np.log(y_predicted) + (1 - y) * np.log(1 - y_predicted))
            self.costs.append(cost)
            
            # 计算梯度
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)
            
            # 更新参数
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X, threshold=0.5):
        linear_model = np.dot(X, self.weights) + self.bias
        y_predicted = self._sigmoid(linear_model)
        predictions = [1 if i > threshold else 0 for i in y_predicted]
        return np.array(predictions)
```

## 无监督学习算法

### K-Means 聚类

```python
class KMeansFromScratch:
    def __init__(self, k=3, max_iters=100, random_state=None):
        self.k = k
        self.max_iters = max_iters
        self.random_state = random_state
        
    def _initialize_centroids(self, X):
        if self.random_state:
            np.random.seed(self.random_state)
        
        n_samples, n_features = X.shape
        centroids = np.zeros((self.k, n_features))
        
        for i in range(self.k):
            centroid = X[np.random.choice(n_samples)]
            centroids[i] = centroid
            
        return centroids
    
    def _closest_centroid(self, sample, centroids):
        distances = [np.linalg.norm(sample - centroid) for centroid in centroids]
        return np.argmin(distances)
    
    def fit(self, X):
        centroids = self._initialize_centroids(X)
        
        for _ in range(self.max_iters):
            clusters = [[] for _ in range(self.k)]
            
            for idx, sample in enumerate(X):
                centroid_idx = self._closest_centroid(sample, centroids)
                clusters[centroid_idx].append(idx)
            
            previous_centroids = centroids.copy()
            
            for idx, cluster in enumerate(clusters):
                if cluster:
                    centroids[idx] = np.mean(X[cluster], axis=0)
            
            if np.allclose(previous_centroids, centroids):
                break
        
        self.centroids = centroids
        return self
```

## 集成学习

### 随机森林实现

```python
from collections import Counter

class RandomForestFromScratch:
    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.n_features = n_features
        self.trees = []
    
    def _bootstrap_samples(self, X, y):
        n_samples = X.shape[0]
        idxs = np.random.choice(n_samples, n_samples, replace=True)
        return X[idxs], y[idxs]
    
    def _most_common_label(self, y):
        counter = Counter(y)
        most_common = counter.most_common(1)[0][0]
        return most_common
    
    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            tree = DecisionTree(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                n_features=self.n_features
            )
            X_sample, y_sample = self._bootstrap_samples(X, y)
            tree.fit(X_sample, y_sample)
            self.trees.append(tree)
    
    def predict(self, X):
        predictions = np.array([tree.predict(X) for tree in self.trees])
        tree_preds = np.swapaxes(predictions, 0, 1)
        predictions = np.array([self._most_common_label(pred) for pred in tree_preds])
        return predictions
```

## 深度学习基础

### 神经网络实现

```python
class NeuralNetwork:
    def __init__(self, layers):
        self.layers = layers
        self.weights = []
        self.biases = []
        
        # 初始化权重和偏置
        for i in range(len(layers) - 1):
            w = np.random.randn(layers[i], layers[i + 1]) * 0.1
            b = np.zeros((1, layers[i + 1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def _sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def _sigmoid_derivative(self, x):
        return x * (1 - x)
    
    def forward(self, X):
        self.activations = [X]
        
        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            a = self._sigmoid(z)
            self.activations.append(a)
        
        return self.activations[-1]
    
    def backward(self, X, y, learning_rate):
        m = X.shape[0]
        
        # 计算输出层误差
        delta = self.activations[-1] - y
        
        # 反向传播
        for i in range(len(self.weights) - 1, -1, -1):
            # 计算梯度
            dW = np.dot(self.activations[i].T, delta) / m
            db = np.sum(delta, axis=0, keepdims=True) / m
            
            # 更新权重和偏置
            self.weights[i] -= learning_rate * dW
            self.biases[i] -= learning_rate * db
            
            # 计算下一层误差
            if i > 0:
                delta = np.dot(delta, self.weights[i].T) * self._sigmoid_derivative(self.activations[i])
    
    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            # 前向传播
            output = self.forward(X)
            
            # 反向传播
            self.backward(X, y, learning_rate)
            
            # 计算损失
            if epoch % 100 == 0:
                loss = np.mean((output - y) ** 2)
                print(f'Epoch {epoch}, Loss: {loss:.4f}')
    
    def predict(self, X):
        return self.forward(X)
```

## 模型评估与优化

### 交叉验证

```python
from sklearn.model_selection import KFold

def cross_validate_model(model, X, y, cv=5):
    kf = KFold(n_splits=cv, shuffle=True, random_state=42)
    scores = []
    
    for train_idx, val_idx in kf.split(X):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        
        accuracy = np.mean(y_pred == y_val)
        scores.append(accuracy)
    
    return np.mean(scores), np.std(scores)
```

### 超参数调优

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

def hyperparameter_tuning(X, y):
    # 定义参数网格
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    # 创建模型
    rf = RandomForestClassifier(random_state=42)
    
    # 网格搜索
    grid_search = GridSearchCV(
        rf, param_grid, cv=5, 
        scoring='accuracy', n_jobs=-1
    )
    
    grid_search.fit(X, y)
    
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best score: {grid_search.best_score_:.3f}")
    
    return grid_search.best_estimator_
```

## 实际应用案例

### 房价预测

```python
# 加载数据
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler

# 数据预处理
boston = load_boston()
X, y = boston.data, boston.target

# 特征缩放
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 分割数据
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 训练模型
lr_model = LinearRegressionFromScratch(learning_rate=0.01, n_iterations=1000)
lr_model.fit(X_train, y_train)

# 预测和评估
y_pred = lr_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"R²: {r2:.3f}")
```

## 总结

机器学习算法实战的核心要点：

1. **数据预处理** - 清洗、特征工程和标准化
2. **算法选择** - 根据问题类型选择合适算法
3. **模型训练** - 参数优化和正则化
4. **模型评估** - 交叉验证和性能指标
5. **超参数调优** - 网格搜索和贝叶斯优化
6. **模型部署** - 生产环境部署和监控

掌握这些技能将让你能够构建高质量的机器学习应用。